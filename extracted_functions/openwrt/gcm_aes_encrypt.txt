int gcm_aes_encrypt(struct aead_request *req)
{
    struct aes_ctx *ctx = crypto_tfm_ctx(req->base.tfm);
    struct skcipher_walk walk;
    struct skcipher_request request;
    int err;
    unsigned int enc_bytes, nbytes;
    be128 lengths;
    u8 iv[AES_BLOCK_SIZE];

    lengths.a = cpu_to_be64(req->assoclen * 8);
    lengths.b = cpu_to_be64(req->cryptlen * 8);

    memset(ctx->hash, 0, sizeof(ctx->hash));
    memset(ctx->block, 0, sizeof(ctx->block));
    memcpy(iv, req->iv, GCM_AES_IV_SIZE);
    *(__be32 *)((void *)iv + GCM_AES_IV_SIZE) = cpu_to_be32(1);
    ifx_deu_aes_ctr(ctx, ctx->block, ctx->block,
                       iv, 16, CRYPTO_DIR_ENCRYPT, 0);

    request.cryptlen = req->cryptlen + req->assoclen;
    request.src = req->src;
    request.dst = req->dst;
    request.base = req->base;

    crypto_skcipher_alg(crypto_skcipher_reqtfm(&request))->walksize = AES_BLOCK_SIZE;

    if (req->assoclen && (req->assoclen < AES_BLOCK_SIZE))
        crypto_skcipher_alg(crypto_skcipher_reqtfm(&request))->walksize = req->assoclen;

    err = skcipher_walk_virt(&walk, &request, false);

    //process assoc data if available
    if (req->assoclen > 0) {
        unsigned int assoc_remain, ghashlen;

        assoc_remain = req->assoclen;
        ghashlen = min(req->assoclen, walk.nbytes);
        while ((nbytes = enc_bytes = ghashlen) && (ghashlen >= AES_BLOCK_SIZE)) {
            u8 *temp;
            if (nbytes > req->assoclen) nbytes = enc_bytes = req->assoclen;
            enc_bytes -= (nbytes % AES_BLOCK_SIZE);
            memcpy(walk.dst.virt.addr, walk.src.virt.addr, enc_bytes);
            assoc_remain -= enc_bytes;
            temp = walk.dst.virt.addr;
            while (enc_bytes > 0) {
                u128_xor((u128 *)ctx->hash, (u128 *)ctx->hash, (u128 *)temp);
                gf128mul_4k_lle((be128 *)ctx->hash, ctx->gf128);
                enc_bytes -= AES_BLOCK_SIZE;
                temp += 16;
            }
            if (assoc_remain < AES_BLOCK_SIZE) walk.stride = assoc_remain;
            if (assoc_remain == 0) walk.stride = AES_BLOCK_SIZE;
            enc_bytes = nbytes - (nbytes % AES_BLOCK_SIZE);
            err = skcipher_walk_done(&walk, (walk.nbytes - enc_bytes));
            ghashlen = min(assoc_remain, walk.nbytes);
        }

        if ((enc_bytes = ghashlen)) {
            memcpy(ctx->lastbuffer, walk.src.virt.addr, enc_bytes);
            memset(ctx->lastbuffer + enc_bytes, 0, (AES_BLOCK_SIZE - enc_bytes));
            memcpy(walk.dst.virt.addr, walk.src.virt.addr, ghashlen);
            u128_xor((u128 *)ctx->hash, (u128 *)ctx->hash, (u128 *)ctx->lastbuffer);
            gf128mul_4k_lle((be128 *)ctx->hash, ctx->gf128);
            walk.stride = AES_BLOCK_SIZE;
            err = skcipher_walk_done(&walk, (walk.nbytes - ghashlen));
        }
    }

    //crypt and hash
    while ((nbytes = enc_bytes = walk.nbytes) && (walk.nbytes >= AES_BLOCK_SIZE)) {
        u8 *temp;
        enc_bytes -= (nbytes % AES_BLOCK_SIZE);
        ifx_deu_aes_ctr(ctx, walk.dst.virt.addr, walk.src.virt.addr,
                       iv, enc_bytes, CRYPTO_DIR_ENCRYPT, 0);
        nbytes &= AES_BLOCK_SIZE - 1;
        temp = walk.dst.virt.addr;
        while (enc_bytes) {
            u128_xor((u128 *)ctx->hash, (u128 *)ctx->hash, (u128 *)temp);
            gf128mul_4k_lle((be128 *)ctx->hash, ctx->gf128);
            enc_bytes -= AES_BLOCK_SIZE;
            temp += 16;
        }
        err = skcipher_walk_done(&walk, nbytes);
    }

    /* crypt and hash remaining bytes < AES_BLOCK_SIZE */
    if ((enc_bytes = walk.nbytes)) {
        ifx_deu_aes_ctr(ctx, walk.dst.virt.addr, walk.src.virt.addr,
                       iv, walk.nbytes, CRYPTO_DIR_ENCRYPT, 0);
        memcpy(ctx->lastbuffer, walk.dst.virt.addr, enc_bytes);
        memset(ctx->lastbuffer + enc_bytes, 0, (AES_BLOCK_SIZE - enc_bytes));
        u128_xor((u128 *)ctx->hash, (u128 *)ctx->hash, (u128 *)ctx->lastbuffer);
        gf128mul_4k_lle((be128 *)ctx->hash, ctx->gf128);
        err = skcipher_walk_done(&walk, 0);
    }

    //finalize and copy hash
    u128_xor((u128 *)ctx->hash, (u128 *)ctx->hash, (u128 *)&lengths);
    gf128mul_4k_lle((be128 *)ctx->hash, ctx->gf128);
    u128_xor((u128 *)ctx->hash, (u128 *)ctx->hash, (u128 *)ctx->block);
    scatterwalk_map_and_copy(ctx->hash, req->dst, req->cryptlen + req->assoclen, crypto_aead_authsize(crypto_aead_reqtfm(req)), 1);

    aead_request_complete(req, 0);

    return err;
}
